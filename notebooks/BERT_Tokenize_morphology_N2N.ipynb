{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 한국어 Tokenizing (Morphology) End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import collections, re, unicodedata, six, csv, os, copy, json, math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\"Input의 고유 id, text A and B, label을 기록하는 객체\"\"\"\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"Input의 embedding feature 및 label, real sample 여부를 기록하는 객체\"\"\"\n",
    "    def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example\n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Processor 모체 클래스\"\"\"\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"tab 구분자로 구분된 파일을 읽는 메서드\"\"\"\n",
    "        with tf.gfile.Open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "class SmishingProcessor(DataProcessor):\n",
    "    \"\"\"DataProcessor를 상속받아 InputExample 객체의 list를 생성하는 객체\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = convert_to_unicode(line[2])\n",
    "            if set_type == \"test\":\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                label = convert_to_unicode(line[-1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples\n",
    "    \n",
    "def convert_to_unicode(text):\n",
    "    if six.PY3: # Python3일 경우 여기\n",
    "        if isinstance(text, str):\n",
    "            # string일 경우 그대로 반환\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            # bytes일 경우 utf-8로 decoding\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            # 이 외의 타입은 지원하지 않는다.\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2: # Python2일 경우 여기\n",
    "        if isinstance(text, str):\n",
    "            # string일 경우 utf-8로 decoding\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            # unicode 타입일 경우 그대로 반환\n",
    "            return text\n",
    "        else:\n",
    "            # 이 외의 타입은 지원하지 않는다.\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else: # Python2, Python3 둘 다 아닐 경우 raise ValueError.\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 미리 setting\n",
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Jupyter notebook에서 사용시에만 필요\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "## Required parameters\n",
    "flags.DEFINE_string(\n",
    "    \"data_dir\", None,\n",
    "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
    "    \"for the task.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_predict\", False,\n",
    "    \"Whether to run the model in inference mode on the test set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output_dir/smishing/train.tf_record'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dacon_path = '../dacon문자스미싱/filedown (2)/'\n",
    "train_file = os.path.join('./output_dir/smishing/', 'train.tf_record')\n",
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SmishingProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# get train samples\n",
    "train_examples = processor.get_train_examples(dacon_path)\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(XXX고객님 안녕하세요 유익한 어플 세가지 소개해드리고자 합니다.1. 스타알림 : 6개의 비밀번호 만으로 거래내역조회 가능 입출금통지가 무료 (창구에서는 한달에 900원 or 한건에 20원)2. 리브 : USJPEU 환전우대 90% 번호표 미리 뽑기 지인에게 로그인없이 바로 이체 창구에서 통장없이 바로 출금송금3. 리브메이트 : 잠자고 있는 카드포인트리 조회  계좌로 현금화 송금어플 설치시 직원번호 XXX 꼭 넣어주세요 .언제나 XXX은행을 이용해 주셔서 감사합니다.XXX은행판교종합금융센터XXX올림XXX-XXX-XXX무료수신거부XXX-XXX-XXX'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[5].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn: file_based_convert_examples_to_features\n",
    "\n",
    "## Arguments\n",
    "examples = train_examples\n",
    "label_list = label_list\n",
    "max_seq_length = FLAGS.max_seq_length\n",
    "# tokenizer = FullTokenizer() # 예시로 tokenizing을 어떻게 하는지 전부 기록\n",
    "output_file = train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_index = 5\n",
    "example = examples[ex_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fn: convert_single_example\n",
    "isinstance(example, PaddingInputExample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0, '1': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(/SSO XXX/SL 고객/NNG 님/XSN 안녕/NNG 하/XSV 세요/EP+EF 유익/XR 한/XSA+ETM 어/IC 플/NNG 세/MM 가지/NNBC 소개/NNG 해/XSV+EC 드리/VX 고자/EC 합니다/VX+EF ./SF 1/SN ./SF 스타/NNG 알림/VV+ETN :/SC 6/SN 개/NNBC 의/JKG 비밀/NNG 번호/NNG 만/JX 으로/JKB 거래/NNG 내역/NNG 조회/NNG 가능/NNG 입출금/NNG 통지/NNG 가/JKS 무료/NNG (/SSO 창구/NNG 에서/JKB 는/JX 한/MM 달/NNG 에/JKB 900/SN 원/NNBC or/SL 한/MM 건/NNBC 에/JKB 20/SN 원/NNBC )/SSC 2/SN ./SF 리브/NNG :/SC USJPEU/SL 환전/NNG 우대/NNG 90/SN %/SY 번호표/NNG 미리/MAG 뽑/VV 기/ETN 지인/NNG 에게/JKB 로그인/NNG 없이/MAG 바로/MAG 이체/NNG 창구/NNG 에서/JKB 통장/NNG 없이/MAG 바로/MAG 출금/NNG 송금/NNG 3/SN ./SF 리브/NNG 메이트/NNG :/SC 잠자/VV 고/EC 있/VX 는/ETM 카드/NNG 포/NNG 인/VCP+ETM 트리/NNG 조회/NNG 계좌/NNG 로/JKB 현금화/NNG 송금/NNG 어/IC 플/NNG 설치/VV 시/EP 직원/NNG 번호/NNG XXX/SL 꼭/MAG 넣/VV 어/EC 주/VX 세요/EP+EF ./SF 언제나/MAG XXX/SL 은행/NNG 을/JKO 이용/NNG 해/XSV+EC 주/VX 셔서/EP+EC 감사/NNG 합니다/XSV+EF ./SF XXX/SL 은행/NNG 판교/NNG 종합/NNG 금융/NNG 센터/NNG XXX/SL 올림/VV+EC XXX/SL -/SY XXX/SL -/SY XXX/SL 무료/NNG 수신/NNG 거부/NNG XXX/SL -/SY XXX/SL -/SY XXX/SL'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class: FullTokenizer\n",
    "path = '../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/'\n",
    "FLAGS.vocab_file = path + 'vocab.korean_morp.list' \n",
    "vocab_file = FLAGS.vocab_file\n",
    "do_lower_case = FLAGS.do_lower_case\n",
    "text = example.text_a\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "\n",
    "            ### joonho.lim @ 2019-03-15\n",
    "            if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "                continue\n",
    "            token = token.split('\\t')[0]\n",
    "\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "inv_vocab = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x00', '�')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0), chr(0xfffd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(text):\n",
    "    output = [] # char을 저장할 list 생성\n",
    "    for char in text:\n",
    "        # 텍스트에서 Char 단위로 출력\n",
    "        cp = ord(char)\n",
    "        if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "            # \\x00이거나 �이거나 unicode cat.이 C로 시작할 경우\n",
    "            # (개행문자 제외) output에 추가하지 않는다.\n",
    "            continue\n",
    "        if _is_whitespace(char):\n",
    "            # 공백일 경우 \" \"으로 output에 추가\n",
    "            output.append(\" \")\n",
    "        else:\n",
    "            # 이 외의 경우 전부 output에 추가\n",
    "            output.append(char)\n",
    "    # cleaning 작업을 거친 Text를 후처리하여 반환\n",
    "    return \"\".join(output)\n",
    "\n",
    "# char 단위 함수들\n",
    "def _is_whitespace(char):\n",
    "    if char == \" \" or char == '\\t' or char == '\\n' or char == '\\r':\n",
    "        # 개행문자이거나 띄어쓰기면 True 반환\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat == 'Zs':\n",
    "        # unicode category가 Space Seperator면 True 반환\n",
    "        return True\n",
    "    # 이 외의 경우 전부 False 반환\n",
    "    return False\n",
    "\n",
    "def _is_control(char):\n",
    "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "        # 개행문자이면 False 반환\n",
    "        return False\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"C\"):\n",
    "        # unicode category가\n",
    "        # Cc(Control) \n",
    "        # Cf(format)\n",
    "        # Co(Private Use, is 0)\n",
    "        # Cs(Surrrogate, is 0)일 경우, True 반환\n",
    "        return True\n",
    "    # 이 외의 경우 전부 False 반환\n",
    "    return False\n",
    "\n",
    "def _is_punctuation(char):\n",
    "    # 한국어 형태소 분석기이기 때문에 공백과 같은지 여부만 반환\n",
    "    return char == ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(/SSO XXX/SL 고객/NNG 님/XSN 안녕/NNG 하/XSV 세요/EP+EF 유익/XR 한/XSA+ETM 어/IC 플/NNG 세/MM 가지/NNBC 소개/NNG 해/XSV+EC 드리/VX 고자/EC 합니다/VX+EF ./SF 1/SN ./SF 스타/NNG 알림/VV+ETN :/SC 6/SN 개/NNBC 의/JKG 비밀/NNG 번호/NNG 만/JX 으로/JKB 거래/NNG 내역/NNG 조회/NNG 가능/NNG 입출금/NNG 통지/NNG 가/JKS 무료/NNG (/SSO 창구/NNG 에서/JKB 는/JX 한/MM 달/NNG 에/JKB 900/SN 원/NNBC or/SL 한/MM 건/NNBC 에/JKB 20/SN 원/NNBC )/SSC 2/SN ./SF 리브/NNG :/SC USJPEU/SL 환전/NNG 우대/NNG 90/SN %/SY 번호표/NNG 미리/MAG 뽑/VV 기/ETN 지인/NNG 에게/JKB 로그인/NNG 없이/MAG 바로/MAG 이체/NNG 창구/NNG 에서/JKB 통장/NNG 없이/MAG 바로/MAG 출금/NNG 송금/NNG 3/SN ./SF 리브/NNG 메이트/NNG :/SC 잠자/VV 고/EC 있/VX 는/ETM 카드/NNG 포/NNG 인/VCP+ETM 트리/NNG 조회/NNG 계좌/NNG 로/JKB 현금화/NNG 송금/NNG 어/IC 플/NNG 설치/VV 시/EP 직원/NNG 번호/NNG XXX/SL 꼭/MAG 넣/VV 어/EC 주/VX 세요/EP+EF ./SF 언제나/MAG XXX/SL 은행/NNG 을/JKO 이용/NNG 해/XSV+EC 주/VX 셔서/EP+EC 감사/NNG 합니다/XSV+EF ./SF XXX/SL 은행/NNG 판교/NNG 종합/NNG 금융/NNG 센터/NNG XXX/SL 올림/VV+EC XXX/SL -/SY XXX/SL -/SY XXX/SL 무료/NNG 수신/NNG 거부/NNG XXX/SL -/SY XXX/SL -/SY XXX/SL'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if i % (20 // 4) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_c(text, is_print):\n",
    "    if is_print:\n",
    "        print(text)\n",
    "    else:\n",
    "        print(end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** START TOKENING MORPHLOGY **************\n",
      "\n",
      "Origin Text:   (/SSO XXX/SL 고객/NNG 님/XSN 안녕/NNG 하/XSV 세요/EP+EF 유익/XR 한/XSA+ETM 어/IC 플/NNG 세/MM 가지/NNBC 소개/NNG 해/XSV+EC 드리/VX 고자/EC 합니다/VX+EF ./SF 1/SN ./SF 스타/NNG 알림/VV+ETN :/SC 6/SN 개/NNBC 의/JKG 비밀/NNG 번호/NNG 만/JX 으로/JKB 거래/NNG 내역/NNG 조회/NNG 가능/NNG 입출금/NNG 통지/NNG 가/JKS 무료/NNG (/SSO 창구/NNG 에서/JKB 는/JX 한/MM 달/NNG 에/JKB 900/SN 원/NNBC or/SL 한/MM 건/NNBC 에/JKB 20/SN 원/NNBC )/SSC 2/SN ./SF 리브/NNG :/SC USJPEU/SL 환전/NNG 우대/NNG 90/SN %/SY 번호표/NNG 미리/MAG 뽑/VV 기/ETN 지인/NNG 에게/JKB 로그인/NNG 없이/MAG 바로/MAG 이체/NNG 창구/NNG 에서/JKB 통장/NNG 없이/MAG 바로/MAG 출금/NNG 송금/NNG 3/SN ./SF 리브/NNG 메이트/NNG :/SC 잠자/VV 고/EC 있/VX 는/ETM 카드/NNG 포/NNG 인/VCP+ETM 트리/NNG 조회/NNG 계좌/NNG 로/JKB 현금화/NNG 송금/NNG 어/IC 플/NNG 설치/VV 시/EP 직원/NNG 번호/NNG XXX/SL 꼭/MAG 넣/VV 어/EC 주/VX 세요/EP+EF ./SF 언제나/MAG XXX/SL 은행/NNG 을/JKO 이용/NNG 해/XSV+EC 주/VX 셔서/EP+EC 감사/NNG 합니다/XSV+EF ./SF XXX/SL 은행/NNG 판교/NNG 종합/NNG 금융/NNG 센터/NNG XXX/SL 올림/VV+EC XXX/SL -/SY XXX/SL -/SY XXX/SL 무료/NNG 수신/NNG 거부/NNG XXX/SL -/SY XXX/SL -/SY XXX/SL\n",
      "\n",
      "Cleaned Text:  (/SSO XXX/SL 고객/NNG 님/XSN 안녕/NNG 하/XSV 세요/EP+EF 유익/XR 한/XSA+ETM 어/IC 플/NNG 세/MM 가지/NNBC 소개/NNG 해/XSV+EC 드리/VX 고자/EC 합니다/VX+EF ./SF 1/SN ./SF 스타/NNG 알림/VV+ETN :/SC 6/SN 개/NNBC 의/JKG 비밀/NNG 번호/NNG 만/JX 으로/JKB 거래/NNG 내역/NNG 조회/NNG 가능/NNG 입출금/NNG 통지/NNG 가/JKS 무료/NNG (/SSO 창구/NNG 에서/JKB 는/JX 한/MM 달/NNG 에/JKB 900/SN 원/NNBC or/SL 한/MM 건/NNBC 에/JKB 20/SN 원/NNBC )/SSC 2/SN ./SF 리브/NNG :/SC USJPEU/SL 환전/NNG 우대/NNG 90/SN %/SY 번호표/NNG 미리/MAG 뽑/VV 기/ETN 지인/NNG 에게/JKB 로그인/NNG 없이/MAG 바로/MAG 이체/NNG 창구/NNG 에서/JKB 통장/NNG 없이/MAG 바로/MAG 출금/NNG 송금/NNG 3/SN ./SF 리브/NNG 메이트/NNG :/SC 잠자/VV 고/EC 있/VX 는/ETM 카드/NNG 포/NNG 인/VCP+ETM 트리/NNG 조회/NNG 계좌/NNG 로/JKB 현금화/NNG 송금/NNG 어/IC 플/NNG 설치/VV 시/EP 직원/NNG 번호/NNG XXX/SL 꼭/MAG 넣/VV 어/EC 주/VX 세요/EP+EF ./SF 언제나/MAG XXX/SL 은행/NNG 을/JKO 이용/NNG 해/XSV+EC 주/VX 셔서/EP+EC 감사/NNG 합니다/XSV+EF ./SF XXX/SL 은행/NNG 판교/NNG 종합/NNG 금융/NNG 센터/NNG XXX/SL 올림/VV+EC XXX/SL -/SY XXX/SL -/SY XXX/SL 무료/NNG 수신/NNG 거부/NNG XXX/SL -/SY XXX/SL -/SY XXX/SL\n",
      "\n",
      "Orig. Tokens:  ['(/SSO', 'XXX/SL', '고객/NNG', '님/XSN', '안녕/NNG', '하/XSV', '세요/EP+EF', '유익/XR', '한/XSA+ETM', '어/IC', '플/NNG', '세/MM', '가지/NNBC', '소개/NNG', '해/XSV+EC', '드리/VX', '고자/EC', '합니다/VX+EF', './SF', '1/SN', './SF', '스타/NNG', '알림/VV+ETN', ':/SC', '6/SN', '개/NNBC', '의/JKG', '비밀/NNG', '번호/NNG', '만/JX', '으로/JKB', '거래/NNG', '내역/NNG', '조회/NNG', '가능/NNG', '입출금/NNG', '통지/NNG', '가/JKS', '무료/NNG', '(/SSO', '창구/NNG', '에서/JKB', '는/JX', '한/MM', '달/NNG', '에/JKB', '900/SN', '원/NNBC', 'or/SL', '한/MM', '건/NNBC', '에/JKB', '20/SN', '원/NNBC', ')/SSC', '2/SN', './SF', '리브/NNG', ':/SC', 'USJPEU/SL', '환전/NNG', '우대/NNG', '90/SN', '%/SY', '번호표/NNG', '미리/MAG', '뽑/VV', '기/ETN', '지인/NNG', '에게/JKB', '로그인/NNG', '없이/MAG', '바로/MAG', '이체/NNG', '창구/NNG', '에서/JKB', '통장/NNG', '없이/MAG', '바로/MAG', '출금/NNG', '송금/NNG', '3/SN', './SF', '리브/NNG', '메이트/NNG', ':/SC', '잠자/VV', '고/EC', '있/VX', '는/ETM', '카드/NNG', '포/NNG', '인/VCP+ETM', '트리/NNG', '조회/NNG', '계좌/NNG', '로/JKB', '현금화/NNG', '송금/NNG', '어/IC', '플/NNG', '설치/VV', '시/EP', '직원/NNG', '번호/NNG', 'XXX/SL', '꼭/MAG', '넣/VV', '어/EC', '주/VX', '세요/EP+EF', './SF', '언제나/MAG', 'XXX/SL', '은행/NNG', '을/JKO', '이용/NNG', '해/XSV+EC', '주/VX', '셔서/EP+EC', '감사/NNG', '합니다/XSV+EF', './SF', 'XXX/SL', '은행/NNG', '판교/NNG', '종합/NNG', '금융/NNG', '센터/NNG', 'XXX/SL', '올림/VV+EC', 'XXX/SL', '-/SY', 'XXX/SL', '-/SY', 'XXX/SL', '무료/NNG', '수신/NNG', '거부/NNG', 'XXX/SL', '-/SY', 'XXX/SL', '-/SY', 'XXX/SL']\n",
      "\n",
      "orig Token : (/SSO\n",
      "\tstripped accent+norm(NFD) Token : (/sso\n",
      "\tchars : ['(', '/', 's', 's', 'o']\n",
      "\t\t[['(']]\n",
      "\t\t[['(', '/']]\n",
      "\t\t[['(', '/', 's']]\n",
      "\t\t[['(', '/', 's', 's']]\n",
      "\t\t[['(', '/', 's', 's', 'o']]\n",
      "\t['(/sso']\n",
      "orig Token : 번호/NNG\n",
      "\tstripped accent+norm(NFD) Token : 번호/nng\n",
      "\tchars : ['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'n', 'n', 'g']\n",
      "\t\t[['ᄇ']]\n",
      "\t\t[['ᄇ', 'ᅥ']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'n']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'n', 'n']]\n",
      "\t\t[['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'n', 'n', 'g']]\n",
      "\t['번호/nng']\n",
      "orig Token : ./SF\n",
      "\tstripped accent+norm(NFD) Token : ./sf\n",
      "\tchars : ['.', '/', 's', 'f']\n",
      "\t\t[['.']]\n",
      "\t\t[['.', '/']]\n",
      "\t\t[['.', '/', 's']]\n",
      "\t\t[['.', '/', 's', 'f']]\n",
      "\t['./sf']\n",
      "orig Token : 메이트/NNG\n",
      "\tstripped accent+norm(NFD) Token : 메이트/nng\n",
      "\tchars : ['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ', '/', 'n', 'n', 'g']\n",
      "\t\t[['ᄆ']]\n",
      "\t\t[['ᄆ', 'ᅦ']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ', '/']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ', '/', 'n']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ', '/', 'n', 'n']]\n",
      "\t\t[['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ', '/', 'n', 'n', 'g']]\n",
      "\t['메이트/nng']\n",
      "orig Token : 언제나/MAG\n",
      "\tstripped accent+norm(NFD) Token : 언제나/mag\n",
      "\tchars : ['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ', '/', 'm', 'a', 'g']\n",
      "\t\t[['ᄋ']]\n",
      "\t\t[['ᄋ', 'ᅥ']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ', '/']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ', '/', 'm']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ', '/', 'm', 'a']]\n",
      "\t\t[['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ', '/', 'm', 'a', 'g']]\n",
      "\t['언제나/mag']\n",
      "orig Token : -/SY\n",
      "\tstripped accent+norm(NFD) Token : -/sy\n",
      "\tchars : ['-', '/', 's', 'y']\n",
      "\t\t[['-']]\n",
      "\t\t[['-', '/']]\n",
      "\t\t[['-', '/', 's']]\n",
      "\t\t[['-', '/', 's', 'y']]\n",
      "\t['-/sy']\n",
      "\n",
      "split_tokens :  ['(/sso', 'xxx/sl', '고객/nng', '님/xsn', '안녕/nng', '하/xsv', '세요/ep+ef', '유익/xr', '한/xsa+etm', '어/ic', '플/nng', '세/mm', '가지/nnbc', '소개/nng', '해/xsv+ec', '드리/vx', '고자/ec', '합니다/vx+ef', './sf', '1/sn', './sf', '스타/nng', '알림/vv+etn', ':/sc', '6/sn', '개/nnbc', '의/jkg', '비밀/nng', '번호/nng', '만/jx', '으로/jkb', '거래/nng', '내역/nng', '조회/nng', '가능/nng', '입출금/nng', '통지/nng', '가/jks', '무료/nng', '(/sso', '창구/nng', '에서/jkb', '는/jx', '한/mm', '달/nng', '에/jkb', '900/sn', '원/nnbc', 'or/sl', '한/mm', '건/nnbc', '에/jkb', '20/sn', '원/nnbc', ')/ssc', '2/sn', './sf', '리브/nng', ':/sc', 'usjpeu/sl', '환전/nng', '우대/nng', '90/sn', '%/sy', '번호표/nng', '미리/mag', '뽑/vv', '기/etn', '지인/nng', '에게/jkb', '로그인/nng', '없이/mag', '바로/mag', '이체/nng', '창구/nng', '에서/jkb', '통장/nng', '없이/mag', '바로/mag', '출금/nng', '송금/nng', '3/sn', './sf', '리브/nng', '메이트/nng', ':/sc', '잠자/vv', '고/ec', '있/vx', '는/etm', '카드/nng', '포/nng', '인/vcp+etm', '트리/nng', '조회/nng', '계좌/nng', '로/jkb', '현금화/nng', '송금/nng', '어/ic', '플/nng', '설치/vv', '시/ep', '직원/nng', '번호/nng', 'xxx/sl', '꼭/mag', '넣/vv', '어/ec', '주/vx', '세요/ep+ef', './sf', '언제나/mag', 'xxx/sl', '은행/nng', '을/jko', '이용/nng', '해/xsv+ec', '주/vx', '셔서/ep+ec', '감사/nng', '합니다/xsv+ef', './sf', 'xxx/sl', '은행/nng', '판교/nng', '종합/nng', '금융/nng', '센터/nng', 'xxx/sl', '올림/vv+ec', 'xxx/sl', '-/sy', 'xxx/sl', '-/sy', 'xxx/sl', '무료/nng', '수신/nng', '거부/nng', 'xxx/sl', '-/sy', 'xxx/sl', '-/sy', 'xxx/sl']\n",
      "\n",
      "_____WHITE_SPACING_____\n",
      "\n",
      "output Tokens:  ['(/sso', 'xxx/sl', '고객/nng', '님/xsn', '안녕/nng', '하/xsv', '세요/ep+ef', '유익/xr', '한/xsa+etm', '어/ic', '플/nng', '세/mm', '가지/nnbc', '소개/nng', '해/xsv+ec', '드리/vx', '고자/ec', '합니다/vx+ef', './sf', '1/sn', './sf', '스타/nng', '알림/vv+etn', ':/sc', '6/sn', '개/nnbc', '의/jkg', '비밀/nng', '번호/nng', '만/jx', '으로/jkb', '거래/nng', '내역/nng', '조회/nng', '가능/nng', '입출금/nng', '통지/nng', '가/jks', '무료/nng', '(/sso', '창구/nng', '에서/jkb', '는/jx', '한/mm', '달/nng', '에/jkb', '900/sn', '원/nnbc', 'or/sl', '한/mm', '건/nnbc', '에/jkb', '20/sn', '원/nnbc', ')/ssc', '2/sn', './sf', '리브/nng', ':/sc', 'usjpeu/sl', '환전/nng', '우대/nng', '90/sn', '%/sy', '번호표/nng', '미리/mag', '뽑/vv', '기/etn', '지인/nng', '에게/jkb', '로그인/nng', '없이/mag', '바로/mag', '이체/nng', '창구/nng', '에서/jkb', '통장/nng', '없이/mag', '바로/mag', '출금/nng', '송금/nng', '3/sn', './sf', '리브/nng', '메이트/nng', ':/sc', '잠자/vv', '고/ec', '있/vx', '는/etm', '카드/nng', '포/nng', '인/vcp+etm', '트리/nng', '조회/nng', '계좌/nng', '로/jkb', '현금화/nng', '송금/nng', '어/ic', '플/nng', '설치/vv', '시/ep', '직원/nng', '번호/nng', 'xxx/sl', '꼭/mag', '넣/vv', '어/ec', '주/vx', '세요/ep+ef', './sf', '언제나/mag', 'xxx/sl', '은행/nng', '을/jko', '이용/nng', '해/xsv+ec', '주/vx', '셔서/ep+ec', '감사/nng', '합니다/xsv+ef', './sf', 'xxx/sl', '은행/nng', '판교/nng', '종합/nng', '금융/nng', '센터/nng', 'xxx/sl', '올림/vv+ec', 'xxx/sl', '-/sy', 'xxx/sl', '-/sy', 'xxx/sl', '무료/nng', '수신/nng', '거부/nng', 'xxx/sl', '-/sy', 'xxx/sl', '-/sy', 'xxx/sl']\n",
      "\n",
      "['(', '/', 's', 's', 'o', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['ᄀ', 'ᅩ', 'ᄀ', 'ᅢ', 'ᆨ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄂ', 'ᅵ', 'ᆷ', '/', 'x', 's', 'n', '_']\n",
      "['ᄋ', 'ᅡ', 'ᆫ', 'ᄂ', 'ᅧ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄒ', 'ᅡ', '/', 'x', 's', 'v', '_']\n",
      "['ᄉ', 'ᅦ', 'ᄋ', 'ᅭ', '/', 'e', 'p', '+', 'e', 'f', '_']\n",
      "['ᄋ', 'ᅲ', 'ᄋ', 'ᅵ', 'ᆨ', '/', 'x', 'r', '_']\n",
      "['ᄒ', 'ᅡ', 'ᆫ', '/', 'x', 's', 'a', '+', 'e', 't', 'm', '_']\n",
      "['ᄋ', 'ᅥ', '/', 'i', 'c', '_']\n",
      "['ᄑ', 'ᅳ', 'ᆯ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄉ', 'ᅦ', '/', 'm', 'm', '_']\n",
      "['ᄀ', 'ᅡ', 'ᄌ', 'ᅵ', '/', 'n', 'n', 'b', 'c', '_']\n",
      "['ᄉ', 'ᅩ', 'ᄀ', 'ᅢ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄒ', 'ᅢ', '/', 'x', 's', 'v', '+', 'e', 'c', '_']\n",
      "['ᄃ', 'ᅳ', 'ᄅ', 'ᅵ', '/', 'v', 'x', '_']\n",
      "['ᄀ', 'ᅩ', 'ᄌ', 'ᅡ', '/', 'e', 'c', '_']\n",
      "['ᄒ', 'ᅡ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'v', 'x', '+', 'e', 'f', '_']\n",
      "['.', '/', 's', 'f', '_']\n",
      "['1', '/', 's', 'n', '_']\n",
      "['.', '/', 's', 'f', '_']\n",
      "['ᄉ', 'ᅳ', 'ᄐ', 'ᅡ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅡ', 'ᆯ', 'ᄅ', 'ᅵ', 'ᆷ', '/', 'v', 'v', '+', 'e', 't', 'n', '_']\n",
      "[':', '/', 's', 'c', '_']\n",
      "['6', '/', 's', 'n', '_']\n",
      "['ᄀ', 'ᅢ', '/', 'n', 'n', 'b', 'c', '_']\n",
      "['ᄋ', 'ᅴ', '/', 'j', 'k', 'g', '_']\n",
      "['ᄇ', 'ᅵ', 'ᄆ', 'ᅵ', 'ᆯ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄆ', 'ᅡ', 'ᆫ', '/', 'j', 'x', '_']\n",
      "['ᄋ', 'ᅳ', 'ᄅ', 'ᅩ', '/', 'j', 'k', 'b', '_']\n",
      "['ᄀ', 'ᅥ', 'ᄅ', 'ᅢ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄂ', 'ᅢ', 'ᄋ', 'ᅧ', 'ᆨ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄌ', 'ᅩ', 'ᄒ', 'ᅬ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄀ', 'ᅡ', 'ᄂ', 'ᅳ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅵ', 'ᆸ', 'ᄎ', 'ᅮ', 'ᆯ', 'ᄀ', 'ᅳ', 'ᆷ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄐ', 'ᅩ', 'ᆼ', 'ᄌ', 'ᅵ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄀ', 'ᅡ', '/', 'j', 'k', 's', '_']\n",
      "['ᄆ', 'ᅮ', 'ᄅ', 'ᅭ', '/', 'n', 'n', 'g', '_']\n",
      "['(', '/', 's', 's', 'o', '_']\n",
      "['ᄎ', 'ᅡ', 'ᆼ', 'ᄀ', 'ᅮ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅦ', 'ᄉ', 'ᅥ', '/', 'j', 'k', 'b', '_']\n",
      "['ᄂ', 'ᅳ', 'ᆫ', '/', 'j', 'x', '_']\n",
      "['ᄒ', 'ᅡ', 'ᆫ', '/', 'm', 'm', '_']\n",
      "['ᄃ', 'ᅡ', 'ᆯ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅦ', '/', 'j', 'k', 'b', '_']\n",
      "['9', '0', '0', '/', 's', 'n', '_']\n",
      "['ᄋ', 'ᅯ', 'ᆫ', '/', 'n', 'n', 'b', 'c', '_']\n",
      "['o', 'r', '/', 's', 'l', '_']\n",
      "['ᄒ', 'ᅡ', 'ᆫ', '/', 'm', 'm', '_']\n",
      "['ᄀ', 'ᅥ', 'ᆫ', '/', 'n', 'n', 'b', 'c', '_']\n",
      "['ᄋ', 'ᅦ', '/', 'j', 'k', 'b', '_']\n",
      "['2', '0', '/', 's', 'n', '_']\n",
      "['ᄋ', 'ᅯ', 'ᆫ', '/', 'n', 'n', 'b', 'c', '_']\n",
      "[')', '/', 's', 's', 'c', '_']\n",
      "['2', '/', 's', 'n', '_']\n",
      "['.', '/', 's', 'f', '_']\n",
      "['ᄅ', 'ᅵ', 'ᄇ', 'ᅳ', '/', 'n', 'n', 'g', '_']\n",
      "[':', '/', 's', 'c', '_']\n",
      "['u', 's', 'j', 'p', 'e', 'u', '/', 's', 'l', '_']\n",
      "['ᄒ', 'ᅪ', 'ᆫ', 'ᄌ', 'ᅥ', 'ᆫ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅮ', 'ᄃ', 'ᅢ', '/', 'n', 'n', 'g', '_']\n",
      "['9', '0', '/', 's', 'n', '_']\n",
      "['%', '/', 's', 'y', '_']\n",
      "['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', 'ᄑ', 'ᅭ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄆ', 'ᅵ', 'ᄅ', 'ᅵ', '/', 'm', 'a', 'g', '_']\n",
      "['ᄈ', 'ᅩ', 'ᆸ', '/', 'v', 'v', '_']\n",
      "['ᄀ', 'ᅵ', '/', 'e', 't', 'n', '_']\n",
      "['ᄌ', 'ᅵ', 'ᄋ', 'ᅵ', 'ᆫ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅦ', 'ᄀ', 'ᅦ', '/', 'j', 'k', 'b', '_']\n",
      "['ᄅ', 'ᅩ', 'ᄀ', 'ᅳ', 'ᄋ', 'ᅵ', 'ᆫ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅥ', 'ᆹ', 'ᄋ', 'ᅵ', '/', 'm', 'a', 'g', '_']\n",
      "['ᄇ', 'ᅡ', 'ᄅ', 'ᅩ', '/', 'm', 'a', 'g', '_']\n",
      "['ᄋ', 'ᅵ', 'ᄎ', 'ᅦ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄎ', 'ᅡ', 'ᆼ', 'ᄀ', 'ᅮ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅦ', 'ᄉ', 'ᅥ', '/', 'j', 'k', 'b', '_']\n",
      "['ᄐ', 'ᅩ', 'ᆼ', 'ᄌ', 'ᅡ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅥ', 'ᆹ', 'ᄋ', 'ᅵ', '/', 'm', 'a', 'g', '_']\n",
      "['ᄇ', 'ᅡ', 'ᄅ', 'ᅩ', '/', 'm', 'a', 'g', '_']\n",
      "['ᄎ', 'ᅮ', 'ᆯ', 'ᄀ', 'ᅳ', 'ᆷ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄉ', 'ᅩ', 'ᆼ', 'ᄀ', 'ᅳ', 'ᆷ', '/', 'n', 'n', 'g', '_']\n",
      "['3', '/', 's', 'n', '_']\n",
      "['.', '/', 's', 'f', '_']\n",
      "['ᄅ', 'ᅵ', 'ᄇ', 'ᅳ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄆ', 'ᅦ', 'ᄋ', 'ᅵ', 'ᄐ', 'ᅳ', '/', 'n', 'n', 'g', '_']\n",
      "[':', '/', 's', 'c', '_']\n",
      "['ᄌ', 'ᅡ', 'ᆷ', 'ᄌ', 'ᅡ', '/', 'v', 'v', '_']\n",
      "['ᄀ', 'ᅩ', '/', 'e', 'c', '_']\n",
      "['ᄋ', 'ᅵ', 'ᆻ', '/', 'v', 'x', '_']\n",
      "['ᄂ', 'ᅳ', 'ᆫ', '/', 'e', 't', 'm', '_']\n",
      "['ᄏ', 'ᅡ', 'ᄃ', 'ᅳ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄑ', 'ᅩ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅵ', 'ᆫ', '/', 'v', 'c', 'p', '+', 'e', 't', 'm', '_']\n",
      "['ᄐ', 'ᅳ', 'ᄅ', 'ᅵ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄌ', 'ᅩ', 'ᄒ', 'ᅬ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄀ', 'ᅨ', 'ᄌ', 'ᅪ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄅ', 'ᅩ', '/', 'j', 'k', 'b', '_']\n",
      "['ᄒ', 'ᅧ', 'ᆫ', 'ᄀ', 'ᅳ', 'ᆷ', 'ᄒ', 'ᅪ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄉ', 'ᅩ', 'ᆼ', 'ᄀ', 'ᅳ', 'ᆷ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅥ', '/', 'i', 'c', '_']\n",
      "['ᄑ', 'ᅳ', 'ᆯ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄉ', 'ᅥ', 'ᆯ', 'ᄎ', 'ᅵ', '/', 'v', 'v', '_']\n",
      "['ᄉ', 'ᅵ', '/', 'e', 'p', '_']\n",
      "['ᄌ', 'ᅵ', 'ᆨ', 'ᄋ', 'ᅯ', 'ᆫ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', 'ᅩ', '/', 'n', 'n', 'g', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['ᄁ', 'ᅩ', 'ᆨ', '/', 'm', 'a', 'g', '_']\n",
      "['ᄂ', 'ᅥ', 'ᇂ', '/', 'v', 'v', '_']\n",
      "['ᄋ', 'ᅥ', '/', 'e', 'c', '_']\n",
      "['ᄌ', 'ᅮ', '/', 'v', 'x', '_']\n",
      "['ᄉ', 'ᅦ', 'ᄋ', 'ᅭ', '/', 'e', 'p', '+', 'e', 'f', '_']\n",
      "['.', '/', 's', 'f', '_']\n",
      "['ᄋ', 'ᅥ', 'ᆫ', 'ᄌ', 'ᅦ', 'ᄂ', 'ᅡ', '/', 'm', 'a', 'g', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['ᄋ', 'ᅳ', 'ᆫ', 'ᄒ', 'ᅢ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄋ', 'ᅳ', 'ᆯ', '/', 'j', 'k', 'o', '_']\n",
      "['ᄋ', 'ᅵ', 'ᄋ', 'ᅭ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄒ', 'ᅢ', '/', 'x', 's', 'v', '+', 'e', 'c', '_']\n",
      "['ᄌ', 'ᅮ', '/', 'v', 'x', '_']\n",
      "['ᄉ', 'ᅧ', 'ᄉ', 'ᅥ', '/', 'e', 'p', '+', 'e', 'c', '_']\n",
      "['ᄀ', 'ᅡ', 'ᆷ', 'ᄉ', 'ᅡ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄒ', 'ᅡ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '/', 'x', 's', 'v', '+', 'e', 'f', '_']\n",
      "['.', '/', 's', 'f', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['ᄋ', 'ᅳ', 'ᆫ', 'ᄒ', 'ᅢ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄑ', 'ᅡ', 'ᆫ', 'ᄀ', 'ᅭ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄌ', 'ᅩ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᆸ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄀ', 'ᅳ', 'ᆷ', 'ᄋ', 'ᅲ', 'ᆼ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄉ', 'ᅦ', 'ᆫ', 'ᄐ', 'ᅥ', '/', 'n', 'n', 'g', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['ᄋ', 'ᅩ', 'ᆯ', 'ᄅ', 'ᅵ', 'ᆷ', '/', 'v', 'v', '+', 'e', 'c', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['-', '/', 's', 'y', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['-', '/', 's', 'y', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['ᄆ', 'ᅮ', 'ᄅ', 'ᅭ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄉ', 'ᅮ', 'ᄉ', 'ᅵ', 'ᆫ', '/', 'n', 'n', 'g', '_']\n",
      "['ᄀ', 'ᅥ', 'ᄇ', 'ᅮ', '/', 'n', 'n', 'g', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['-', '/', 's', 'y', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "['-', '/', 's', 'y', '_']\n",
      "['x', 'x', 'x', '/', 's', 'l', '_']\n",
      "\n",
      "split_tokens :  ['[UNK]', '/', 's', 's', 'o', '_', 'x', 'x', 'x', '/', 's', 'l', '_', 'ᄀ', '[UNK]', 'ᄀ', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄂ', '[UNK]', 'ᆷ', '/', 'x', 's', 'n', '_', '[UNK]', '[UNK]', 'ᆫ', 'ᄂ', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', 'ᄒ', '[UNK]', '/', 'x', 's', 'v', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'e', 'p', '[UNK]', 'e', 'f', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'x', 'r', '_', 'ᄒ', '[UNK]', 'ᆫ', '/', 'x', 's', 'a', '[UNK]', 'e', 't', 'm', '_', '[UNK]', 'ᅥ', '/', 'i', 'c', '_', '[UNK]', '[UNK]', 'ᆯ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '/', 'm', 'm', '_', 'ᄀ', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'b', 'c', '_', '[UNK]', '[UNK]', 'ᄀ', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄒ', '[UNK]', '/', 'x', 's', 'v', '[UNK]', 'e', 'c', '_', 'ᄃ', '[UNK]', '[UNK]', '[UNK]', '/', 'v', 'x', '_', 'ᄀ', '[UNK]', '[UNK]', '[UNK]', '/', 'e', 'c', '_', 'ᄒ', '[UNK]', '[UNK]', 'ᄂ', '[UNK]', 'ᄃ', '[UNK]', '/', 'v', 'x', '[UNK]', 'e', 'f', '_', '.', '/', 's', 'f', '_', '1', '/', 's', 'n', '_', '.', '/', 's', 'f', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆯ', '[UNK]', '[UNK]', 'ᆷ', '/', 'v', 'v', '[UNK]', 'e', 't', 'n', '_', ':', '/', 's', 'c', '_', '6', '/', 's', 'n', '_', 'ᄀ', '[UNK]', '/', 'n', 'n', 'b', 'c', '_', '[UNK]', '[UNK]', '/', 'j', 'k', 'g', '_', 'ᄇ', '[UNK]', '[UNK]', '[UNK]', 'ᆯ', '/', 'n', 'n', 'g', '_', 'ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆫ', '/', 'j', 'x', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'j', 'k', 'b', '_', 'ᄀ', 'ᅥ', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄂ', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᄒ', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄀ', '[UNK]', 'ᄂ', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ᆯ', 'ᄀ', '[UNK]', 'ᆷ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆼ', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄀ', '[UNK]', '/', 'j', 'k', 's', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '/', 's', 's', 'o', '_', '[UNK]', '[UNK]', 'ᆼ', 'ᄀ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '[UNK]', 'ᅥ', '/', 'j', 'k', 'b', '_', 'ᄂ', '[UNK]', 'ᆫ', '/', 'j', 'x', '_', 'ᄒ', '[UNK]', 'ᆫ', '/', 'm', 'm', '_', 'ᄃ', '[UNK]', 'ᆯ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '/', 'j', 'k', 'b', '_', '9', '0', '0', '/', 's', 'n', '_', '[UNK]', '[UNK]', 'ᆫ', '/', 'n', 'n', 'b', 'c', '_', 'o', 'r', '/', 's', 'l', '_', 'ᄒ', '[UNK]', 'ᆫ', '/', 'm', 'm', '_', 'ᄀ', 'ᅥ', 'ᆫ', '/', 'n', 'n', 'b', 'c', '_', '[UNK]', '[UNK]', '/', 'j', 'k', 'b', '_', '2', '0', '/', 's', 'n', '_', '[UNK]', '[UNK]', 'ᆫ', '/', 'n', 'n', 'b', 'c', '_', '[UNK]', '/', 's', 's', 'c', '_', '2', '/', 's', 'n', '_', '.', '/', 's', 'f', '_', '[UNK]', '[UNK]', 'ᄇ', '[UNK]', '/', 'n', 'n', 'g', '_', ':', '/', 's', 'c', '_', 'u', 's', 'j', 'p', 'e', 'u', '/', 's', 'l', '_', 'ᄒ', '[UNK]', 'ᆫ', '[UNK]', 'ᅥ', 'ᆫ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᄃ', '[UNK]', '/', 'n', 'n', 'g', '_', '9', '0', '/', 's', 'n', '_', '%', '/', 's', 'y', '_', 'ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'm', 'a', 'g', '_', '[UNK]', '[UNK]', '[UNK]', '/', 'v', 'v', '_', 'ᄀ', '[UNK]', '/', 'e', 't', 'n', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ᆫ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᄀ', '[UNK]', '/', 'j', 'k', 'b', '_', '[UNK]', '[UNK]', 'ᄀ', '[UNK]', '[UNK]', '[UNK]', 'ᆫ', '/', 'n', 'n', 'g', '_', '[UNK]', 'ᅥ', '[UNK]', '[UNK]', '[UNK]', '/', 'm', 'a', 'g', '_', 'ᄇ', '[UNK]', '[UNK]', '[UNK]', '/', 'm', 'a', 'g', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆼ', 'ᄀ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '[UNK]', 'ᅥ', '/', 'j', 'k', 'b', '_', '[UNK]', '[UNK]', 'ᆼ', '[UNK]', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', '[UNK]', 'ᅥ', '[UNK]', '[UNK]', '[UNK]', '/', 'm', 'a', 'g', '_', 'ᄇ', '[UNK]', '[UNK]', '[UNK]', '/', 'm', 'a', 'g', '_', '[UNK]', '[UNK]', 'ᆯ', 'ᄀ', '[UNK]', 'ᆷ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆼ', 'ᄀ', '[UNK]', 'ᆷ', '/', 'n', 'n', 'g', '_', '3', '/', 's', 'n', '_', '.', '/', 's', 'f', '_', '[UNK]', '[UNK]', 'ᄇ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', ':', '/', 's', 'c', '_', '[UNK]', '[UNK]', 'ᆷ', '[UNK]', '[UNK]', '/', 'v', 'v', '_', 'ᄀ', '[UNK]', '/', 'e', 'c', '_', '[UNK]', '[UNK]', '[UNK]', '/', 'v', 'x', '_', 'ᄂ', '[UNK]', 'ᆫ', '/', 'e', 't', 'm', '_', '[UNK]', '[UNK]', 'ᄃ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆫ', '/', 'v', 'c', 'p', '[UNK]', 'e', 't', 'm', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᄒ', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄀ', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '/', 'j', 'k', 'b', '_', 'ᄒ', '[UNK]', 'ᆫ', 'ᄀ', '[UNK]', 'ᆷ', 'ᄒ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆼ', 'ᄀ', '[UNK]', 'ᆷ', '/', 'n', 'n', 'g', '_', '[UNK]', 'ᅥ', '/', 'i', 'c', '_', '[UNK]', '[UNK]', 'ᆯ', '/', 'n', 'n', 'g', '_', '[UNK]', 'ᅥ', 'ᆯ', '[UNK]', '[UNK]', '/', 'v', 'v', '_', '[UNK]', '[UNK]', '/', 'e', 'p', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ᆫ', '/', 'n', 'n', 'g', '_', 'ᄇ', 'ᅥ', 'ᆫ', 'ᄒ', '[UNK]', '/', 'n', 'n', 'g', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '[UNK]', '[UNK]', '[UNK]', '/', 'm', 'a', 'g', '_', 'ᄂ', 'ᅥ', '[UNK]', '/', 'v', 'v', '_', '[UNK]', 'ᅥ', '/', 'e', 'c', '_', '[UNK]', '[UNK]', '/', 'v', 'x', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'e', 'p', '[UNK]', 'e', 'f', '_', '.', '/', 's', 'f', '_', '[UNK]', 'ᅥ', 'ᆫ', '[UNK]', '[UNK]', 'ᄂ', '[UNK]', '/', 'm', 'a', 'g', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '[UNK]', '[UNK]', 'ᆫ', 'ᄒ', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆯ', '/', 'j', 'k', 'o', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', 'ᄒ', '[UNK]', '/', 'x', 's', 'v', '[UNK]', 'e', 'c', '_', '[UNK]', '[UNK]', '/', 'v', 'x', '_', '[UNK]', '[UNK]', '[UNK]', 'ᅥ', '/', 'e', 'p', '[UNK]', 'e', 'c', '_', 'ᄀ', '[UNK]', 'ᆷ', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄒ', '[UNK]', '[UNK]', 'ᄂ', '[UNK]', 'ᄃ', '[UNK]', '/', 'x', 's', 'v', '[UNK]', 'e', 'f', '_', '.', '/', 's', 'f', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '[UNK]', '[UNK]', 'ᆫ', 'ᄒ', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆫ', 'ᄀ', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆼ', 'ᄒ', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', 'ᄀ', '[UNK]', 'ᆷ', '[UNK]', '[UNK]', 'ᆼ', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', 'ᆫ', '[UNK]', 'ᅥ', '/', 'n', 'n', 'g', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '[UNK]', '[UNK]', 'ᆯ', '[UNK]', '[UNK]', 'ᆷ', '/', 'v', 'v', '[UNK]', 'e', 'c', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '-', '/', 's', 'y', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '-', '/', 's', 'y', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '/', 'n', 'n', 'g', '_', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'ᆫ', '/', 'n', 'n', 'g', '_', 'ᄀ', 'ᅥ', 'ᄇ', '[UNK]', '/', 'n', 'n', 'g', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '-', '/', 's', 'y', '_', 'x', 'x', 'x', '/', 's', 'l', '_', '-', '/', 's', 'y', '_', 'x', 'x', 'x', '/', 's', 'l', '_']\n"
     ]
    }
   ],
   "source": [
    "# FullTokenizer.tokenize(); End2End Tokenizer\n",
    "text = example.text_a # text 초기화\n",
    "print('************** START TOKENING MORPHLOGY **************')\n",
    "split_tokens = [] \n",
    "# BasicTokenizer.tokenize()\n",
    "print('\\nOrigin Text:  ', text)\n",
    "text = convert_to_unicode(text)\n",
    "text = _clean_text(text)\n",
    "print('\\nCleaned Text: ', text)\n",
    "# fn: whitespace_tokenize()\n",
    "text = text.strip()\n",
    "if not text: orig_tokens = []\n",
    "else: orig_tokens = text.split()\n",
    "print('\\nOrig. Tokens: ', orig_tokens, end='\\n\\n')\n",
    "split_tokens = []\n",
    "for (ix, token) in enumerate(orig_tokens):\n",
    "    if ix % (len(orig_tokens) // 5) == 0:\n",
    "        # 전체 token 중 10개만 tokenizing 결과 출력\n",
    "        is_print = True\n",
    "    else:\n",
    "        # 아닐 경우 출력하지 않는다.\n",
    "        is_print = False\n",
    "    print_c('orig Token : '+token, is_print=is_print)\n",
    "    if do_lower_case: # True\n",
    "        token = token.lower()\n",
    "        # fn: _run_strip_accents\n",
    "        t = unicodedata.normalize(\"NFD\", token)\n",
    "        # https://gist.github.com/Pusnow/aa865fa21f9557fa58d691a8b79f8a6d\n",
    "        # 모든 음절을 정준 분해(Canonical Decomposition)시킴\n",
    "        # '각'을 'ㄱ+ㅏ+ㄱ'으로 저장(출력되는 값은 동일)\n",
    "        output = []\n",
    "        for char in t:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Mn\":\n",
    "                # unicode category가 \"Mark, Nonspacing\"일 경우 pass\n",
    "                continue\n",
    "            output.append(char)\n",
    "        token = \"\".join(output)\n",
    "        print_c('\\tstripped accent+norm(NFD) Token : '+t, is_print=is_print)\n",
    "    # fn: _run_split_on_punc()\n",
    "    chars = list(token)\n",
    "    i, start_new_word = 0, True\n",
    "    output = []\n",
    "    print_c('\\tchars : '+str(chars), is_print)\n",
    "    while i < len(chars):\n",
    "        char = chars[i]\n",
    "        if _is_punctuation(char): # == \" \"과 같은 표현\n",
    "            output.append([char])\n",
    "            start_new_word = True\n",
    "        else:\n",
    "            if start_new_word:\n",
    "                output.append([])\n",
    "            start_new_word = False\n",
    "            output[-1].append(char)\n",
    "        i += 1\n",
    "        print_c('\\t\\t' + str(output), is_print=is_print)\n",
    "    token = [\"\".join(x) for x in output]\n",
    "    print_c('\\t' + str(token), is_print=is_print)\n",
    "    split_tokens.extend(token)\n",
    "print('\\nsplit_tokens : ', split_tokens)\n",
    "# fn: whitespace_tokenize\n",
    "print('\\n_____WHITE_SPACING_____')\n",
    "split_tokens = (' '.join(split_tokens)).strip()\n",
    "if not split_tokens: output_tokens = []\n",
    "else: output_tokens = split_tokens.split()\n",
    "print('\\noutput Tokens: ', output_tokens, end='\\n\\n')\n",
    "\n",
    "split_tokens = [] # 최종 결과물 저장할 리스트\n",
    "for tokens in output_tokens:\n",
    "    tokens += '_' # adding '_'\n",
    "    # WordpieceTokenizer.tokenize()\n",
    "    unk_token = \"[UNK]\"\n",
    "    max_input_chars_per_word = 200\n",
    "    # greedy longest-match-first algorithm to perform tokenization\n",
    "    # using the given vocabulary\n",
    "    tokens = convert_to_unicode(tokens)\n",
    "    output_tokens_ = []\n",
    "    # fn: whitespace_tokenize\n",
    "    tokens = (' '.join(tokens)).strip()\n",
    "    if not tokens: tokens = []\n",
    "    else: tokens = tokens.split()\n",
    "    # start lmf algorithm!\n",
    "    print(tokens)\n",
    "    for token in tokens:\n",
    "        chars = list(token)\n",
    "        if len(chars) > max_input_chars_per_word: # 200\n",
    "            output_tokens_.append(unk_token)\n",
    "            continue\n",
    "        is_bad = False\n",
    "        start = 0\n",
    "        sub_tokens = []\n",
    "        while start < len(chars):\n",
    "            end = len(chars)\n",
    "            cur_substr = None\n",
    "            while start < end:\n",
    "                substr = \"\".join(chars[start:end])\n",
    "#                 print(substr)\n",
    "                if substr in vocab:\n",
    "                    cur_substr = substr\n",
    "                    break\n",
    "                end -= 1\n",
    "            if cur_substr is None:\n",
    "                is_bad = True\n",
    "                break\n",
    "            sub_tokens.append(cur_substr)\n",
    "            start = end\n",
    "        if is_bad:\n",
    "            output_tokens_.append(unk_token)\n",
    "        else:\n",
    "            output_tokens_.extend(sub_tokens)\n",
    "    for sub_token in output_tokens_:\n",
    "        split_tokens.append(sub_token)\n",
    "print('\\nsplit_tokens : ', split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordpiece는 아닌듯싶다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = example.text_a.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(/SSO', 'XXX/SL', '고객/NNG', '님/XSN', '안녕/NNG', '하/XSV', '세요/EP+EF', '유익/XR', '한/XSA+ETM', '어/IC', '플/NNG', '세/MM', '가지/NNBC', '소개/NNG', '해/XSV+EC', '드리/VX', '고자/EC', '합니다/VX+EF', './SF', '1/SN', './SF', '스타/NNG', '알림/VV+ETN', ':/SC', '6/SN', '개/NNBC', '의/JKG', '비밀/NNG', '번호/NNG', '만/JX', '으로/JKB', '거래/NNG', '내역/NNG', '조회/NNG', '가능/NNG', '입출금/NNG', '통지/NNG', '가/JKS', '무료/NNG', '(/SSO', '창구/NNG', '에서/JKB', '는/JX', '한/MM', '달/NNG', '에/JKB', '900/SN', '원/NNBC', 'or/SL', '한/MM', '건/NNBC', '에/JKB', '20/SN', '원/NNBC', ')/SSC', '2/SN', './SF', '리브/NNG', ':/SC', 'USJPEU/SL', '환전/NNG', '우대/NNG', '90/SN', '%/SY', '번호표/NNG', '미리/MAG', '뽑/VV', '기/ETN', '지인/NNG', '에게/JKB', '로그인/NNG', '없이/MAG', '바로/MAG', '이체/NNG', '창구/NNG', '에서/JKB', '통장/NNG', '없이/MAG', '바로/MAG', '출금/NNG', '송금/NNG', '3/SN', './SF', '리브/NNG', '메이트/NNG', ':/SC', '잠자/VV', '고/EC', '있/VX', '는/ETM', '카드/NNG', '포/NNG', '인/VCP+ETM', '트리/NNG', '조회/NNG', '계좌/NNG', '로/JKB', '현금화/NNG', '송금/NNG', '어/IC', '플/NNG', '설치/VV', '시/EP', '직원/NNG', '번호/NNG', 'XXX/SL', '꼭/MAG', '넣/VV', '어/EC', '주/VX', '세요/EP+EF', './SF', '언제나/MAG', 'XXX/SL', '은행/NNG', '을/JKO', '이용/NNG', '해/XSV+EC', '주/VX', '셔서/EP+EC', '감사/NNG', '합니다/XSV+EF', './SF', 'XXX/SL', '은행/NNG', '판교/NNG', '종합/NNG', '금융/NNG', '센터/NNG', 'XXX/SL', '올림/VV+EC', 'XXX/SL', '-/SY', 'XXX/SL', '-/SY', 'XXX/SL', '무료/NNG', '수신/NNG', '거부/NNG', 'XXX/SL', '-/SY', 'XXX/SL', '-/SY', 'XXX/SL']\n",
      "['(/SSO', 'XXX/SL', '고객/NNG', '님/XSN', '안녕/NNG', '하/XSV', '세요/EP+EF', '유익/XR', '한/XSA+ETM', '어/IC', '플/NNG', '세/MM', '가지/NNBC', '소개/NNG', '해/XSV+EC', '드리/VX', '고자/EC', '합니다/VX+EF', './SF', '1/SN', './SF', '스타/NNG', '알림/VV+ETN', ':/SC', '6/SN', '개/NNBC', '의/JKG', '비밀/NNG', '번호/NNG', '만/JX', '으로/JKB', '거래/NNG', '내역/NNG', '조회/NNG', '가능/NNG', '입출금/NNG', '통지/NNG', '가/JKS', '무료/NNG', '(/SSO', '창구/NNG', '에서/JKB', '는/JX', '한/MM', '달/NNG', '에/JKB', '900/SN', '원/NNBC', 'or/SL', '한/MM', '건/NNBC', '에/JKB', '20/SN', '원/NNBC', ')/SSC', '2/SN', './SF', '리브/NNG', ':/SC', 'USJPEU/SL', '환전/NNG', '우대/NNG', '90/SN', '%/SY', '번호표/NNG', '미리/MAG', '뽑/VV', '기/ETN', '지인/NNG', '에게/JKB', '로그인/NNG', '없이/MAG', '바로/MAG', '이체/NNG', '창구/NNG', '에서/JKB', '통장/NNG', '없이/MAG', '바로/MAG', '출금/NNG', '송금/NNG', '3/SN', './SF', '리브/NNG', '메이트/NNG', ':/SC', '잠자/VV', '고/EC', '있/VX', '는/ETM', '카드/NNG', '포/NNG', '인/VCP+ETM', '트리/NNG', '조회/NNG', '계좌/NNG', '로/JKB', '현금화/NNG', '송금/NNG', '어/IC', '플/NNG', '설치/VV', '시/EP', '직원/NNG', '번호/NNG', 'XXX/SL', '꼭/MAG', '넣/VV', '어/EC', '주/VX', '세요/EP+EF', './SF', '언제나/MAG', 'XXX/SL', '은행/NNG', '을/JKO', '이용/NNG', '해/XSV+EC', '주/VX', '셔서/EP+EC', '감사/NNG', '합니다/XSV+EF', './SF', 'XXX/SL', '은행/NNG', '판교/NNG', '종합/NNG', '금융/NNG', '센터/NNG', 'XXX/SL', '올림/VV+EC', 'XXX/SL', '-/SY', 'XXX/SL', '-/SY', 'XXX/SL', '무료/NNG', '수신/NNG', '거부/NNG', 'XXX/SL', '-/SY', 'XXX/SL', '-/SY', 'XXX/SL']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_a)\n",
    "if len(tokens_a) > max_seq_length - 2:\n",
    "    tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "print(tokens_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
