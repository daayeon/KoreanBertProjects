{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 8\n",
    "PREDICT_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "MAX_SEQ_LENGTH = 64\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments: 11개의 NLP Tasks\n",
    "1. MNLI(Multi-Genre Natural Language Inference): 두 개의 문장을 주고, 두번째 문장이 첫번째 문장과 같은 의미(entailment)인지, 모순(contradiction)이 있는지, 무관한(neutral) 의미인지 판단\n",
    "2. QQP(Quora Question Pair): 두 질문이 같은 의미의 질문인지 판단\n",
    "3. QNLI(Question Natural Language Inference): 질문과 문장을 주고, 그 문장에 답이 있으면 pos, 없으면 neg를 판단\n",
    "4. SST-2(Stanford Sentiment Treebank): movie review data에서 sentiment 분석(binary)\n",
    "5. CoLA(Corpus of Linguistic Acceptability): 문장의 문법이 맞는지 판단\n",
    "6. STS-B(Semantic Textual Similarity Benchmark): 두 문장의 의미의 유사도를 1~5점으로 판단\n",
    "7. MRPC(Microsoft Research Paraphrase corpus): 두 문장의 sentiment가 같은지 판단\n",
    "8. RTE(Recognizing Textual Entailment): MNLI와 비슷\n",
    "9. SQuAD 1.1(Stanford Question Answering Database): 질문을 보고 지문에서 answer text span을 찾아낸다. 즉, 정답의 시작점과 끝점을 찾아냄\n",
    "10. CoNLL Named Entity Recognition: 단어에 Person, Organization, Location, Miscellaneous, Other-Not named entity를 annotate\n",
    "11. SWAG(Situation with Adversarial Generations): video captioning DB에서 추출한 문장 다음에 올 문장으로 알맞은 것은? (4지선다형)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Task data directory: glue_data\\MRPC *****\n",
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 1687-9ECC\n",
      "\n",
      " C:\\workspace\\dacon문자스미싱\\glue_data\\MRPC 디렉터리\n",
      "\n",
      "2019-12-13  오후 04:37    <DIR>          .\n",
      "2019-12-13  오후 04:37    <DIR>          ..\n",
      "2019-12-16  오전 10:53           106,025 dev.tsv\n",
      "2019-12-16  오전 10:53             6,222 dev_ids.tsv\n",
      "2019-12-16  오전 10:57           441,275 msr_paraphrase_test.txt\n",
      "2019-12-16  오전 10:57         1,047,044 msr_paraphrase_train.txt\n",
      "2019-12-16  오전 10:53           447,061 test.tsv\n",
      "2019-12-16  오전 10:53           945,140 train.tsv\n",
      "               6개 파일           2,992,767 바이트\n",
      "               2개 디렉터리  325,955,076,096 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "TASK = 'MRPC' #@param {type:\"string\"}\n",
    "\n",
    "TASK_DATA_DIR = 'glue_data\\\\' + TASK\n",
    "print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))\n",
    "!dir $TASK_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = './bucket/' #@param {type:\"string\"}\n",
    "assert BUCKET, 'Must specify an existing GCS bucket name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './{}/bert-tfhub/models/{}'.format(BUCKET, TASK)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: ././bucket//bert-tfhub/models/MRPC *****\n"
     ]
    }
   ],
   "source": [
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
    "\n",
    "# Available pretrained model checkpoints:\n",
    "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
    "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
    "#   cased_L-12_H-768_A-12: cased BERT large model\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
    "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_MODEL_HUB\t https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path += ['./bert']\n",
    "# In local code, you must be use \"tokenization_wordpiece.py\"\n",
    "import tokenization \n",
    "\n",
    "print('BERT_MODEL_HUB\\t', BERT_MODEL_HUB)\n",
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature='tokenization_info',\n",
    "                                    as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run(\n",
    "            [tokenization_info['vocab_file'],\n",
    "             tokenization_info['do_lower_case']])\n",
    "        \n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, \n",
    "                                       do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "## Required parameters\n",
    "flags.DEFINE_string(\n",
    "    \"data_dir\", None,\n",
    "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
    "    \"for the task.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_predict\", False,\n",
    "    \"Whether to run the model in inference mode on the test set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"\n",
    "        Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "          guid: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"\n",
    "    Fake example so the num input examples is a multiple of the batch size.\n",
    "\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with tf.gfile.Open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n",
    "            \"dev_matched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, tokenization.convert_to_unicode(line[0]))\n",
    "            text_a = tokenization.convert_to_unicode(line[8])\n",
    "            text_b = tokenization.convert_to_unicode(line[9])\n",
    "            if set_type == \"test\":\n",
    "                label = \"contradiction\"\n",
    "            else:\n",
    "                label = tokenization.convert_to_unicode(line[-1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, \n",
    "                             text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MrpcProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = tokenization.convert_to_unicode(line[3])\n",
    "            text_b = tokenization.convert_to_unicode(line[4])\n",
    "            if set_type == \"test\":\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                label = tokenization.convert_to_unicode(line[0])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, \n",
    "                             text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            # Only the test set has a header\n",
    "            if set_type == \"test\" and i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            if set_type == \"test\":\n",
    "                text_a = tokenization.convert_to_unicode(line[1])\n",
    "                label = \"0\"\n",
    "            else:\n",
    "                text_a = tokenization.convert_to_unicode(line[3])\n",
    "                label = tokenization.convert_to_unicode(line[1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, \n",
    "                             text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "  \"cola\": ColaProcessor,\n",
    "  \"mnli\": MnliProcessor,\n",
    "  \"mrpc\": MrpcProcessor,\n",
    "}\n",
    "processor = processors[TASK.lower()]()\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MrpcProcessor at 0x18be2146ba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of train and warmup steps from batch size\n",
    "train_examples = processor.get_train_examples(TASK_DATA_DIR)\n",
    "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-2\n",
      "Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\n",
      "Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(train_examples[i].guid)\n",
    "print(train_examples[i].text_a)\n",
    "print(train_examples[i].text_b)\n",
    "print(train_examples[i].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune and Run Predictions on a pretrained BERT Model from TF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'././bucket//bert-tfhub/models/MRPC'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force TF Hub writes to the GS bucket we provide\n",
    "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_training, input_ids, input_mask,\n",
    "                 segment_ids, labels,\n",
    "                 num_labels, bert_hub_module_handle):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    tag = set()\n",
    "    if is_training:\n",
    "        tags.add('train')\n",
    "    bert_module = hub.Module(bert_hub_module_handle, \n",
    "                             tags=tags, trainable=True)\n",
    "    bert_inputs = dict(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "        inputs=bert_inputs,\n",
    "        signature='tokens',\n",
    "        as_dict=True)\n",
    "    \n",
    "    # In the demo, we are doing a simple classification task on the entire\n",
    "    # segment.\n",
    "    #\n",
    "    # If you want to use the token-level output, use\n",
    "    # bert_outputs[\"sequence_output\"] instead.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "    \n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    output_weights = tf.get_variable(\n",
    "        'output_weights', [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=.02))\n",
    "    \n",
    "    output_bias = tf.get_variable(\n",
    "        'output_bias', [num_labels], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope('loss'):\n",
    "        if is_training:\n",
    "            # i.e., 0.1 drpoout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=.9)\n",
    "            \n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    \n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "    \n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    \n",
    "    return (loss, per_example_loss, logits, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps,\n",
    "                     use_tpu):\n",
    "    \"\"\"Creates an optimizer training op.\"\"\"\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    learing_rate = tf.constant(value=init_lr, shape=[],\n",
    "                               dtype=tf.float32)\n",
    "    \n",
    "    # Implements linear decay of the learning rate.\n",
    "    learning_rate = tf.train.polynomial_decay(\n",
    "        learning_rate=learning_rate,\n",
    "        global_step=global_step,\n",
    "        decay_steps=num_train_steps,\n",
    "        end_learning_rate=0.0,\n",
    "        power=1.0,\n",
    "        cycle=False)\n",
    "    \n",
    "    # Implements linear warmup. I.e., if global_step < num_warmup_steps,\n",
    "    # the learning rate will be 'global_step/num_warmup_steps * init_lr'.\n",
    "    if num_warmup_steps:\n",
    "        global_steps_int = tf.cast(global_step, tf.int32)\n",
    "        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
    "        \n",
    "        global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "        warmup_steps_float = tf.constant(warmup_steps_int, tf.float32)\n",
    "        \n",
    "        warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "        warmup_learning_rate = init_lr * warmup_percent_done\n",
    "        \n",
    "        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.flaot32)\n",
    "        learning_rate = (\n",
    "            (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate\n",
    "        )\n",
    "        \n",
    "    # It is recommended that you use this optimizer for fine tuning, since this\n",
    "    # is how the model was trained (note that the Adam m/v variables are NOT\n",
    "    # loaded from init_checkpoint.)\n",
    "    optimizer = AdamWeightDecayOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay_rate=.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-6,\n",
    "        exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\n",
    "    \n",
    "    if use_tpu:\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "        \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "    \n",
    "    # This is how the model was pre-trained.\n",
    "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "    \n",
    "    train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars), global_step=global_step)\n",
    "    \n",
    "    # Normally the global step update is done inside of 'apply_gradients'.\n",
    "    # However, 'AdamWeightDecayOptimizer' doesn't do this. But if you use\n",
    "    # a different optimizer, you should probably take this line out.\n",
    "    new_global_step = global_step + 1\n",
    "    train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
    "    return train_op\n",
    "\n",
    "class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
    "    \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 learning_rate,\n",
    "                 weight_decay_rate=0.0,\n",
    "                 beta_1=0.9,\n",
    "                 beta_2=0.999,\n",
    "                 epsilon=1e-6,\n",
    "                 exclude_from_weight_decay=None,\n",
    "                 name='AdamWeightDecayOptimizer'):\n",
    "        \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
    "        super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay_rate = weight_decay_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        \n",
    "    def apply_gradients(self, grad_and_vars, global_step=None, name=None):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        assignments = []\n",
    "        for (grad, param) in grads_and_vars:\n",
    "            if grad is None or param is None:\n",
    "                continue\n",
    "            \n",
    "            param_name = self._get_variable_name(param.name)\n",
    "            \n",
    "            m = tf.get_variable(\n",
    "                name=param_name + '/adam_m',\n",
    "                shape=param.shape.as_list(),\n",
    "                dtype=tf.float32,\n",
    "                trainable=False,\n",
    "                initializer=tf.zeros_initializer())\n",
    "            v = tf.get_variable(\n",
    "                name=param_name + '/adam_v',\n",
    "                shape=param.shape.as_list(),\n",
    "                dtype=tf.float32,\n",
    "                trainable=False,\n",
    "                initializer=tf.zeros_initializer())\n",
    "            \n",
    "            # Standard Adam update.\n",
    "            next_m = (\n",
    "                tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, \n",
    "                                                          grad))\n",
    "            next_v = (\n",
    "                tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
    "                                                          tf.square(grad)))\n",
    "            \n",
    "            update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
    "            \n",
    "            # Just adding the square of the weights to the loss function is *not*\n",
    "            # the correct way of using L2 regularization/weight decay with Adam,\n",
    "            # since that will interact with the m and v parameters in strange ways.\n",
    "            #\n",
    "            # Instead we want ot decay the weights in a manner that doesn't interact\n",
    "            # with the m/v parameters. This is equivalent to adding the square\n",
    "            # of the weights to the loss with plain (non-momentum) SGD.\n",
    "            if self._do_use_weight_decay(param_name):\n",
    "                update += self.weight_decay_rate * param\n",
    "                \n",
    "            update_with_lr = self.learning_rate * update\n",
    "            \n",
    "            next_param = param - update_with_lr\n",
    "            \n",
    "            assignments.extend(\n",
    "                [param.assign(next_param),\n",
    "                 m.assign(next_m),\n",
    "                 v.assign(next_v)])\n",
    "        return tf.group(*assignments, name=name)\n",
    "    \n",
    "    def do_use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for 'param_name'.\"\"\"\n",
    "        if not self.weight_decay_rate:\n",
    "            return False\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def _get_variable_name(self, param_name):\n",
    "        \"\"\"Get teh variable name from the tensor name.\"\"\"\n",
    "        m = re.match('^(.*):\\\\d+$', param_name)\n",
    "        if m is not None:\n",
    "            param_name = m.group(1)\n",
    "        return param_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps, use_tpu, bert_hub_module_handle):\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        # pylint: disable=unused-argument\n",
    "        tf.logging.info(\"*** Features ***\")\n",
    "        for name in sorted(features.keys()):\n",
    "            tf.logging.info(\"  name = %s, shape = %s\" %(\n",
    "                name, features[name].shape))\n",
    "            \n",
    "        input_ids = features['input_ids']\n",
    "        input_mask = features['input_mask']\n",
    "        segment_ids = features['segment_ids']\n",
    "        label_ids = features['label_ids']\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKesy.TRAIN)\n",
    "\n",
    "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "            is_training, input_ids, input_mask, segment_ids,\n",
    "            label_ids, num_labels, bert_hub_module_handle)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = create_optimizer(\n",
    "                total_loss, learning_rate, num_train_steps, \n",
    "                num_warmup_steps, use_tpe)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            eval_metrics = {\n",
    "                'eval_accuracy' : tf.metrics.accuracy(\n",
    "                        label_ids,\n",
    "                        tf.argmax(logits, axis=-1,\n",
    "                                  output_type=tf.int32)),\n",
    "                'eval_loss' : tf.metrics.mean(per_example_loss)\n",
    "            }\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops=eval_metrics)\n",
    "        elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={'probabilities':probabilities})\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Only TRAIN, EVAL and PREDICT modes are supported: %s' % (mode))\n",
    "        return output_spec\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    num_labels=len(label_list),\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=False,\n",
    "    bert_hub_module_handle=BERT_MODEL_HUB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_config(output_dir):\n",
    "    return tf.estimator.RunConfig(\n",
    "        model_dir=output_dir,\n",
    "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '././bucket//bert-tfhub/models/MRPC', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018E21033A90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '././bucket//bert-tfhub/models/MRPC', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018E21033A90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    config=get_run_config(OUTPUT_DIR),\n",
    "    params={\n",
    "        'batch_size' : TRAIN_BATCH_SIZE,\n",
    "#         'eval_batch_size' : EVAL_BATCH_SIZE,\n",
    "#         'predict_batch_size' : PREDICT_BATCH_SIZE\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 두 함수는 사용되지 않는다고 한다. (convert_single_example 제외)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is not used by this file but is still used by the Colab and\n",
    "# people who depend on it.\n",
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    all_segment_ids = []\n",
    "    all_label_ids = []\n",
    "\n",
    "    for feature in features:\n",
    "        all_input_ids.append(feature.input_ids)\n",
    "        all_input_mask.append(feature.input_mask)\n",
    "        all_segment_ids.append(feature.segment_ids)\n",
    "        all_label_ids.append(feature.label_id)\n",
    "\n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        num_examples = len(features)\n",
    "\n",
    "        # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "        # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "        # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "        d = tf.data.Dataset.from_tensor_slices({\n",
    "            \"input_ids\":\n",
    "                tf.constant(\n",
    "                     all_input_ids, shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"input_mask\":\n",
    "                tf.constant(\n",
    "                    all_input_mask,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"segment_ids\":\n",
    "                tf.constant(\n",
    "                    all_segment_ids,\n",
    "                    shape=[num_examples, seq_length],\n",
    "                    dtype=tf.int32),\n",
    "            \"label_ids\":\n",
    "                tf.constant(all_label_ids, \n",
    "                            shape=[num_examples], dtype=tf.int32),\n",
    "        })\n",
    "\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "        return d\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example, label_list,\n",
    "                                         max_seq_length, tokenizer)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_id=0,\n",
    "            is_real_example=False)\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "      # Where \"type_ids\" are used to indicate whether this is the first\n",
    "      # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "      # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "      # embedding vector (and position vector). This is not *strictly* necessary\n",
    "      # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "      # it easier for the model to learn the concept of sequences.\n",
    "      #\n",
    "      # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "      # used as the \"sentence vector\". Note that this only makes sense because\n",
    "      # the entire model is fine-tuned.\n",
    "  tokens = []\n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "\n",
    "  if tokens_b:\n",
    "    for token in tokens_b:\n",
    "      tokens.append(token)\n",
    "      segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "  # tokens are attended to.\n",
    "  input_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Zero-pad up to the sequence length.\n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "\n",
    "  label_id = label_map[example.label]\n",
    "  if ex_index < 5:\n",
    "    tf.logging.info(\"*** Example ***\")\n",
    "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "        [tokenization.printable_text(x) for x in tokens]))\n",
    "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "  feature = InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_id,\n",
    "      is_real_example=True)\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def model_train(estimator):\n",
    "    print('MRPC/CoLA on BERT base model normally takes about 2-3 minutes. Please wait...')\n",
    "    # We'll set sequences to be at most 128 tokens long.\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
    "    print('  Num examples = {}'.format(len(train_examples)))\n",
    "    print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "    tf.logging.info('  Num steps = %d', num_train_steps)\n",
    "    train_input_fn = input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "    print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_a : Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence . \n",
      "text_b : Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "\n",
      "tokens_a : ['am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.'] \n",
      "tokens_b : ['referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.']\n",
      "\n",
      "tokens : ['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]'] \n",
      "\n",
      "segment_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "input_ids [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]\n",
      "\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "After ZeroPadding:\n",
      "\n",
      "\tinput_ids : [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\tinput_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\tsegment_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "label_id :  1\n",
      "\n",
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: train-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: train-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . [SEP] referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "ex_index = 0\n",
    "example = train_examples[ex_index]\n",
    "\n",
    "print('text_a :', example.text_a, '\\ntext_b :', example.text_b, end='\\n\\n')\n",
    "\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "tokens_a = tokenizer.tokenize(example.text_a)\n",
    "if example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "if tokens_b:\n",
    "    # MAX_SEQ_LENGTH - 3보다 작게 tokens_a와 tokens_b에서 token 제거\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= MAX_SEQ_LENGTH - 3:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "else:\n",
    "    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n",
    "        tokens_a = tokens_a[0:(MAX_SEQ_LENGTH-2)]\n",
    "\n",
    "print('tokens_a :', tokens_a, '\\ntokens_b :', tokens_b, end='\\n\\n')\n",
    "        \n",
    "tokens = []\n",
    "segment_ids = []\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)\n",
    "# print('tokens :', tokens)\n",
    "# print('segment_ids :', segment_ids)\n",
    "\n",
    "if tokens_b:\n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "print('tokens :', tokens, '\\n\\nsegment_ids :', segment_ids, end='\\n\\n')\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print('input_ids', input_ids)\n",
    "\n",
    "input_mask = [1] * len(input_ids)\n",
    "\n",
    "print('\\ninput_mask', input_mask)\n",
    "\n",
    "# ZERO PADDING\n",
    "while len(input_ids) < MAX_SEQ_LENGTH:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "assert len(input_ids) == MAX_SEQ_LENGTH\n",
    "assert len(input_mask) == MAX_SEQ_LENGTH\n",
    "assert len(segment_ids) == MAX_SEQ_LENGTH\n",
    "\n",
    "print('\\nAfter ZeroPadding:\\n\\n\\tinput_ids : {}\\n\\n\\tinput_mask : {}\\n\\n\\tsegment_ids : {}\\n'.format(\n",
    "    input_ids, input_mask, segment_ids))\n",
    "\n",
    "label_id = label_map[example.label]\n",
    "\n",
    "print('label_id : ', label_id, end='\\n\\n')\n",
    "\n",
    "tf.logging.info('*** Example ***')\n",
    "tf.logging.info('guid: %s' % (example.guid))\n",
    "tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "        [tokenization.printable_text(x) for x in tokens]))\n",
    "tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "train_feature = InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_id,\n",
    "      is_real_example=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Started training at 2019-12-18 09:56:54.945007 *****\n",
      "  Num examples = 3668\n",
      "  Batch size = 16\n",
      "INFO:tensorflow:  Num steps = 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  Num steps = 687\n"
     ]
    }
   ],
   "source": [
    "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_examples)))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info('  Num steps = %d', num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_fn_builder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-6c4ad87eec44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_input_fn = input_fn_builder(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mseq_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     drop_remainder=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_fn_builder' is not defined"
     ]
    }
   ],
   "source": [
    "train_input_fn = input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
