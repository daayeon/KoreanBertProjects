{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Jupyter notebook에서 사용시에만 필요\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "## Required parameters\n",
    "flags.DEFINE_string(\n",
    "    \"data_dir\", None,\n",
    "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
    "    \"for the task.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", None,\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"task_name\", None, \"The name of the task to train.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", None,\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", None,\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 128,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_eval\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_predict\", False,\n",
    "    \"Whether to run the model in inference mode on the test set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 객체 정의\n",
    "class InputExample:\n",
    "    \n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\"\n",
    "    \n",
    "# Feature를 담을 객체 정의\n",
    "class InputFeatures(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataProcessor 정의\n",
    "class DataProcessor(object):\n",
    "    \n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with tf.io.gfile.GFile(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기타 필요한 함수들 정의\n",
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "\n",
    "            ### joonho.lim @ 2019-03-15\n",
    "            if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
    "                continue\n",
    "            token = token.split('\\t')[0]\n",
    "\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def convert_by_vocab(vocab, items):\n",
    "    output = []\n",
    "    for item in items:\n",
    "        output.append(vocab[item])\n",
    "    return output\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    return convert_by_vocab(vocab, tokens)\n",
    "\n",
    "\n",
    "def convert_ids_to_tokens(inv_vocab, ids):\n",
    "    return convert_by_vocab(inv_vocab, ids)\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "def printable_text(text):\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, unicode):\n",
    "            return text.encode(\"utf-8\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(ex_index, example,\n",
    "                           label_list,\n",
    "                           max_seq_length,\n",
    "                           tokenizer,\n",
    "                           openapi_key,\n",
    "                           komoran=None):\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_id=0,\n",
    "            is_real_example=False)\n",
    "\n",
    "    label_map = {label : i for (i, label) in enumerate(label_list)}\n",
    "\n",
    "    if openapi_key is None:\n",
    "        tokens_a = ' '.join(\n",
    "            [i[0] + '/' + i[1]\n",
    "             for i in komoran.pos(example.text_a)])\n",
    "    else:\n",
    "        tokens_a = do_lang(openapi_key, example.text_a)\n",
    "#     if \"openapi error\" in tokens_a:\n",
    "#         tf.logging.info(\"(%d--%s)\" % (ex_index, tokens_a))\n",
    "    tokens_a = tokenizer.tokenize(tokens_a)\n",
    "\n",
    "    if example.text_b:\n",
    "        tokens_b = do_lang(openapi_key, example.text_b)\n",
    "        if \"openapi error\" in tokens_a:\n",
    "            tf.logging.info(\"(%d--%s)\" % (ex_index, tokens_b))\n",
    "            tokens_b = ' '.join(\n",
    "                [i[0] + '/' + i[1]\n",
    "                 for i in Komoran().pos(example.text_b)])\n",
    "        tokens_b = tokenizer.tokenize(tokens_b)\n",
    "\n",
    "        def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "            while True:\n",
    "                total_length = len(tokens_a) + len(tokens_b)\n",
    "                if total_length <= max_length:\n",
    "                    break\n",
    "                if len(tokens_a) > len(tokens_b):\n",
    "                    tokens_a.pop()\n",
    "                else:\n",
    "                    tokens_b.pop()\n",
    "        # Account for [CLS], [SEP],and [SEP] with \"-3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length-3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"-2\"\n",
    "        if len(tokens_a) > max_seq_length-2:\n",
    "            tokens_a = tokens_a[:(max_seq_length-2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    # tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    # type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    # tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    # type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for 'type=0' and\n",
    "    # 'type=1' were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not \"strictly\" necessary\n",
    "    # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "    # if easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "\n",
    "    tokens_b = None\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [\"[SEP]\"]\n",
    "        segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    label_id = label_map[example.label]\n",
    "\n",
    "    if ex_index < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id,\n",
    "        is_real_example=True)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  \"attention_probs_dropout_prob\": 0.1, \n",
    "  \"directionality\": \"bidi\", \n",
    "  \"hidden_act\": \"gelu\", \n",
    "  \"hidden_dropout_prob\": 0.1, \n",
    "  \"hidden_size\": 768, \n",
    "  \"initializer_range\": 0.02, \n",
    "  \"intermediate_size\": 3072, \n",
    "  \"max_position_embeddings\": 512, \n",
    "  \"num_attention_heads\": 12, \n",
    "  \"num_hidden_layers\": 12, \n",
    "  \"pooler_fc_size\": 768, \n",
    "  \"pooler_num_attention_heads\": 12, \n",
    "  \"pooler_num_fc_layers\": 3, \n",
    "  \"pooler_size_per_head\": 128, \n",
    "  \"pooler_type\": \"first_token_transform\", \n",
    "  \"type_vocab_size\": 2, \n",
    "  \"vocab_size\": 30349\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BERT CONGIRUATION>\n",
       "\u0007ddress: 0x26a9bb7e860\n",
       "params:\n",
       "  vocab_size: 30349\n",
       "  hidden_size: 768\n",
       "  num_hidden_layers: 12\n",
       "  num_attention_heads: 12\n",
       "  hidden_act: gelu\n",
       "  intermediate_size: 3072\n",
       "  hidden_dropout_prob: 0.1\n",
       "  attention_probs_dropout_prob: 0.1\n",
       "  max_position_embeddings: 512\n",
       "  type_vocab_size: 2\n",
       "  initializer_range: 0.02\n",
       "  directionality: bidi\n",
       "  pooler_fc_size: 768\n",
       "  pooler_num_attention_heads: 12\n",
       "  pooler_num_fc_layers: 3\n",
       "  pooler_size_per_head: 128\n",
       "  pooler_type: first_token_transform"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERT Config\n",
    "path = '../KorBERT/2_bert_download_002_bert_morp_tensorflow/002_bert_morp_tensorflow/'\n",
    "FLAGS.bert_config_file = path + 'bert_config.json'\n",
    "bert_config = BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     (FLAGS) MAX_SEQ_LENGTH           : 128\n",
      "(BERTConfig) MAX_POSITION_EMBEDDINGS  : 512\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
    "else:\n",
    "    print('     (FLAGS) MAX_SEQ_LENGTH           :', FLAGS.max_seq_length)\n",
    "    print('(BERTConfig) MAX_POSITION_EMBEDDINGS  :', bert_config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not use TPU\n"
     ]
    }
   ],
   "source": [
    "# do not use tpu\n",
    "tpu_cluster_resolver = None\n",
    "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "else:\n",
    "    print('Do not use TPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BROADCAST = 4\n",
    "# PER_HOST_V1 = 2\n",
    "# PER_HOST_V2 = 3\n",
    "# PER_SHARD_V1 = 1\n",
    "# SLICED = 5\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "is_per_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver, # None\n",
    "    master=FLAGS.master, # [Optional] TensorFlow master URL.\n",
    "    model_dir=FLAGS.output_dir, # None\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps, # 1000\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop, # 1000\n",
    "        num_shards=FLAGS.num_tpu_cores, # 8\n",
    "        per_host_input_for_training=is_per_host)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow gpu 사용 가능한지 체크\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 vocab 사전을 등록\n",
    "FLAGS.vocab_file = path + 'vocab.korean_morp.list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAVER NSMC 예제 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paths = [path.replace('\\\\', '/') \n",
    "         for path in glob('../research_persona/nsmc/raw/*.json')]\n",
    "res = []\n",
    "for path in paths:\n",
    "    with open(path, encoding='utf-8') as data_file:\n",
    "        res.extend(json.load(data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "df['y'] = (df.rating.astype(np.int16) > 5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개행문자 제거\n",
    "df.review = df.review.map(lambda x: re.sub('[\\r\\n]', ' ', x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       오경이 아깝다  이런영화 왜 만든거냐\n",
       "date                     11.06.14\n",
       "rating                          3\n",
       "author                   psyc****\n",
       "review_id                 5480540\n",
       "movie_id                    76943\n",
       "y                               0\n",
       "Name: 599278, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[599278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰가 비어있는 열 제거\n",
    "df = df[df.review != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            전체관람가는 아닌것 같아요\n",
       "1                              디렉터스컷으로봐서 거의 3시간짜리인데 참 흡인력있다\n",
       "2         태어나 처음으로 가슴아리는 영화였다.  20년이상 지났지만.. 생각하면  또 가슴이...\n",
       "3         어린시절 고딩때 봤던 때랑 또 결혼하고 나서 봤을때의 느낌은 확실히 다르네요. 뭔가...\n",
       "4         토토에게 넓은 세상을 보여주고픈 알프레도.. 그가 토토를 위해 정을 떼려고 했던 장...\n",
       "                                ...                        \n",
       "712399          아직까지 여운이.... 우연히 봤다가 펑펑 울었네요. 음악때문에 감동이 백배!\n",
       "712400                   쓸쓸한 가을  감성을 채워주는 영화네요.. 사랑하고시프다...\n",
       "712401                            단언컨대 올 가을 최고의 영화!! 장담한다!!\n",
       "712402                    기대없이 봤다가 너무너무 반하게 된 영화! 가을 감성에 딱!\n",
       "712403       나도 모르게 눈물이 주륵주륵! 가을 감성 영화! 강추!!!!OST너무 좋아요!!!!\n",
       "Name: review, Length: 712383, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df['review'],\n",
    "                                                      df['y'],\n",
    "                                                      random_state=42,\n",
    "                                                      test_size=.5,\n",
    "                                                      shuffle=True,\n",
    "                                                      stratify=df['y'])\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid,\n",
    "                                                    y_valid,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=.4,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((356191,), (213715,), (142477,))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((356191,), (213715,), (142477,))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.29999972, 0.20000028])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sets = np.array([356202, 213721, 142481])\n",
    "sets / sum(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat((X_train, y_train), axis=1)\n",
    "df_valid = pd.concat((X_valid, y_valid), axis=1)\n",
    "df_test = pd.concat((X_test, y_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.tsv', sep='\\t')\n",
    "df_valid.to_csv('valid.tsv', sep='\\t')\n",
    "df_test.to_csv('test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataProcessor 제작\n",
    "class NSMCProcessor(DataProcessor):\n",
    "\n",
    "    def get_train_examples(self, data_dir, filename='train.tsv'):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir, filename='valid.tsv'):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"valid\")\n",
    "\n",
    "    def get_test_examples(self, data_dir, filename='test.tsv'):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, filename)), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        try:\n",
    "            for (i, line) in enumerate(lines):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                guid = \"%s-%s\" % (set_type, i)\n",
    "                text_a = convert_to_unicode(line[1])\n",
    "                if set_type == \"test\":\n",
    "                    label = \"0\"\n",
    "                else:\n",
    "                    label = convert_to_unicode(line[-1])\n",
    "                examples.append(\n",
    "                    InputExample(guid=guid, text_a=text_a, label=label))\n",
    "                prev_line = line\n",
    "        except IndexError:\n",
    "            print(i, prev_line)\n",
    "            return None\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    오경이 아깝다  이런영화 왜 만든거냐\n",
       "y                            0\n",
       "Name: 599278, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[599278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    꼬리를 먼저 내린 러시아가 패자인가, 아니면 진정한 용기가 있는 승자인가\n",
       "y                                                1\n",
       "Name: 305153, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NSMCProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# get train samples\n",
    "train_examples = processor.get_train_examples('', 'train.tsv')\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record ETRI model weights\n",
    "FLAGS.init_checkpoint = path + 'model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
